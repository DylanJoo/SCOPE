Encoding nano_beir.fiqa ...
The model arguemnts: ModelArguments(model_name_or_path='meta-llama/Llama-3.1-8B-Instruct', config_name=None, tokenizer_name='meta-llama/Llama-3.1-8B-Instruct', cache_dir=None, pooling='last', normalize=True, temperature=1.0, lora=True, lora_name_or_path='DylanJHJ/repllama-3.1-8b.msmarco-passage.4k/', lora_r=16, lora_alpha=64, lora_dropout=0.1, lora_target_modules='q_proj,k_proj,v_proj,o_proj,down_proj,up_proj,gate_proj', dtype='float32', attn_implementation='flash_attention_2')
